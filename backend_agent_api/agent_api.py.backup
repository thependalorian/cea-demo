from fastapi import FastAPI, HTTPException, BackgroundTasks, Header
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager, nullcontext
from typing import Optional, List, Dict, Any
import json
import asyncio
import traceback
from datetime import datetime
import sys
import os
from pathlib import Path
import base64
import uuid
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Use our centralized import helper
from fix_imports import (
    Message, 
    Conversation, 
    ConversationMessage, 
    UserProfile, 
    Request,
    ConversationStatus,
    ConversationType,
    MessageRole,
    Resume,
    ResumeChunk
)

# Add paths for imports
current_dir = Path(__file__).resolve().parent
rag_pipeline_path = str(current_dir.parent / "rag_pipeline")
if rag_pipeline_path not in sys.path:
    sys.path.append(rag_pipeline_path)

# Remove all the old pydantic_agent import code
# Add pydantic_agent to path if needed
# pydantic_agent_path = str(Path(__file__).resolve().parent.parent / "pydantic_agent")
# if pydantic_agent_path not in sys.path:
#     sys.path.append(pydantic_agent_path)
#     print(f"Added pydantic_agent path to agent_api: {pydantic_agent_path}")

# Try importing from pydantic_agent with fallback
# try:
#     # Import our standardized models
#     from pydantic_agent import (
#         Message, 
#         Conversation, 
#         ConversationMessage, 
#         UserProfile, 
#         Request,
#         ConversationStatus,
#         ConversationType,
#         MessageRole
#     )
#     print("Successfully imported models in agent_api")
# except ImportError as e:
#     print(f"Error importing from pydantic_agent in agent_api: {e}")
#     # Try parent directory
#     parent_pydantic_agent_path = str(Path(__file__).resolve().parent.parent.parent / "pydantic_agent")
#     if parent_pydantic_agent_path not in sys.path:
#         sys.path.append(parent_pydantic_agent_path)
#         print(f"Added parent pydantic_agent path to agent_api: {parent_pydantic_agent_path}")
#     try:
#         from pydantic_agent import (
#             Message, 
#             Conversation, 
#             ConversationMessage, 
#             UserProfile, 
#             Request,
#             ConversationStatus,
#             ConversationType,
#             MessageRole
#         )
#         print("Successfully imported models from parent directory")
#     except ImportError as e2:
#         print(f"Failed to import from parent pydantic_agent in agent_api: {e2}")
#         # Use fallback classes
#         from pydantic import BaseModel
#         from enum import Enum
#         from typing import Optional, Dict, Any, List
# 
#         # Define minimal versions of required classes
#         class MessageRole(str, Enum):
#             SYSTEM = "system"
#             USER = "user"
#             ASSISTANT = "assistant"
#             
#         class ConversationStatus(str, Enum):
#             ACTIVE = "active"
#             ARCHIVED = "archived"
#             
#         class ConversationType(str, Enum):
#             CHAT = "chat"
#             JOB_SEARCH = "job_search"
#             CAREER_ADVICE = "career_advice"
#             
#         class Message(BaseModel):
#             content: str
#             role: MessageRole = MessageRole.USER
#             
#         class ConversationMessage(BaseModel):
#             id: Optional[str] = None
#             conversation_id: str
#             content: str
#             role: MessageRole = MessageRole.USER
#             created_at: Optional[str] = None
#             
#         class Conversation(BaseModel):
#             id: str
#             user_id: str
#             title: Optional[str] = None
#             status: ConversationStatus = ConversationStatus.ACTIVE
#             type: ConversationType = ConversationType.CHAT
#             created_at: Optional[str] = None
#             updated_at: Optional[str] = None
#             
#         class UserProfile(BaseModel):
#             id: str
#             email: Optional[str] = None
#             display_name: Optional[str] = None
#             
#         class Request(BaseModel):
#             query: str
#             user_id: str
#             session_id: Optional[str] = None
#             conversation_id: Optional[str] = None
#             context: Optional[Dict[str, Any]] = None
#             stream: bool = False
#             
#         print("Using fallback model classes")

from clients import get_agent_clients, get_mem0_client_async
from agent import pendo_agent, PendoAgentDeps
from resume_processor import ResumeProcessor
from httpx import AsyncClient

# Import rate limiter
from rate_limiter import RateLimitMiddleware

# Global clients
embedding_client = None
supabase = None
http_client = None
mem0_client = None
resume_processor = None

# File attachment model
class FileAttachment(BaseModel):
    filename: str
    content: str  # base64 encoded
    mime_type: str

# Request model - Using our standardized Request model as a base
class PendoAgentRequest(Request):
    files: Optional[List[FileAttachment]] = None

# Response models - Using our standardized models as a base
class PendoAgentResponse(BaseModel):
    response: str
    session_id: str
    conversation_id: Optional[str] = None
    user_id: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup resources"""
    global embedding_client, supabase, http_client, mem0_client, resume_processor
    
    try:
        print("🌱 Initializing Pendo Climate Economy Assistant...")
        
        # Check for required environment variables
        required_env_vars = [
            "SUPABASE_URL", 
            "SUPABASE_KEY", 
            "OPENAI_API_KEY"
        ]
        
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        
        if missing_vars:
            print(f"❌ Missing required environment variables: {', '.join(missing_vars)}")
            print("Please create a .env file with these variables or set them in your environment")
            raise EnvironmentError(f"Missing environment variables: {', '.join(missing_vars)}")
        
        # Initialize clients
        supabase, embedding_client, http_client = await get_agent_clients()
        mem0_client = await get_mem0_client_async()
        resume_processor = ResumeProcessor()
        
        print("✅ Pendo Assistant initialized successfully")
        yield
        
    except Exception as e:
        print(f"❌ Error during initialization: {e}")
        traceback.print_exc()
        yield
    finally:
        # Cleanup
        if http_client:
            await http_client.aclose()
        print("🔄 Pendo Assistant shutdown complete")

# Initialize FastAPI app
app = FastAPI(
    title="Pendo Climate Economy Assistant API",
    description="AI agent specializing in Massachusetts climate careers and workforce development",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Rate Limiting middleware
app.add_middleware(RateLimitMiddleware)

def stream_error_response(error_message: str, session_id: str = None):
    """Stream error response in SSE format"""
    yield f"data: {json.dumps({'error': error_message, 'session_id': session_id})}\n\n"
    yield "event: end\ndata: {}\n\n"

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "pendo-agent-api",
        "version": "1.0.0"
    }

async def process_file_attachments(files: Optional[List[FileAttachment]], user_id: str = None) -> List[Dict[str, Any]]:
    """Process file attachments and return processed results"""
    if not files:
        return []
    
    processed_files = []
    
    for file in files:
        try:
            # Decode base64 content
            content = base64.b64decode(file.content)
            
            # Process based on mime type
            if file.mime_type.startswith('application/pdf'):
                # Process PDF as resume
                result = await process_resume_file(content, file.filename, user_id)
                processed_files.append({
                    "filename": file.filename,
                    "type": "resume",
                    "result": result
                })
            else:
                processed_files.append({
                    "filename": file.filename,
                    "type": "unsupported",
                    "result": {
                        "error": f"Unsupported file type: {file.mime_type}"
                    }
                })
        except Exception as e:
            processed_files.append({
                "filename": file.filename,
                "type": "error",
                "result": {
                    "error": str(e)
                }
            })
    
    return processed_files

@app.post("/api/pendo-agent")
async def pendo_agent_endpoint(request: PendoAgentRequest, x_openai_api_key: Optional[str] = Header(None)):
    """Main endpoint for the Pendo Climate Economy Assistant"""
    
    # Create a new session ID if not provided
    session_id = request.session_id or str(uuid.uuid4())
    conversation_id = request.conversation_id
    
    # Process any file attachments
    processed_files = await process_file_attachments(request.files, request.user_id)
    
    # DEMO MODE: Use API key from header
    # This is the key provided by the user in the frontend
    custom_api_key = x_openai_api_key
    
    # Initialize new clients with the custom API key - this is crucial
    # so we don't use any environment variables for OpenAI
    temp_supabase, temp_embedding_client, temp_http_client = await get_agent_clients(custom_api_key=custom_api_key)
    
    # Prepare context with file processing results
    context = {
        "processed_files": processed_files,
        "user_id": request.user_id,
        "session_id": session_id,
        "conversation_id": conversation_id,
        "custom_api_key": custom_api_key
    }
    
    # Get a memory client for conversation context
    mem0_client = await get_mem0_client_async()
    
    # Initialize a resume processor if needed
    resume_processor = None
    
    # Create common run context
    run_context = RunContext(
        deps=PendoAgentDeps(
            supabase=temp_supabase,
            embedding_client=temp_embedding_client,
            http_client=temp_http_client,
            mem0_client=mem0_client,
            resume_processor=resume_processor,
            custom_api_key=custom_api_key
        ),
        request={
            "user_id": request.user_id,
            "conversation_id": conversation_id,
            "session_id": session_id,
            "context": context
        }
    )
    
    # Create conversation message
    message = Message(
        id=str(uuid.uuid4()),
        content=request.content,
        role=MessageRole.USER
    )
    
    if request.enable_streaming:
        # Return a streaming response
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream"
        )
    else:
        # Return a non-streaming response
        try:
            response_text = await pendo_agent(
                run_context,
                [message],
                stream=False
            )
            
            return PendoAgentResponse(
                response=response_text,
                session_id=session_id,
                conversation_id=conversation_id,
                user_id=request.user_id
            )
        except Exception as e:
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=str(e))
    
    async def stream_generator():
        """Generate streaming response"""
        try:
            # Get streaming response from agent
            generator = await pendo_agent(
                run_context,
                [message],
                stream=True
            )
            
            # Stream the tokens
            async for token in generator:
                # Format as server-sent event
                data = {
                    "token": token,
                    "session_id": session_id,
                    "conversation_id": conversation_id,
                    "user_id": request.user_id
                }
                yield f"data: {json.dumps(data)}\n\n"
            
            # End the stream
            yield "event: end\ndata: {}\n\n"
            
        except Exception as e:
            # Handle errors in the stream
            logger.error(f"Error in stream: {e}")
            error_data = {
                "error": str(e),
                "session_id": session_id
            }
            yield f"data: {json.dumps(error_data)}\n\n"
            yield "event: end\ndata: {}\n\n"

@app.get("/api/resume/check/{user_id}")
async def check_user_resume(user_id: str):
    """Check if a user has a resume uploaded"""
    try:
        if resume_processor:
            result = resume_processor.check_resume_exists(user_id)
            return result
        else:
            raise HTTPException(status_code=500, detail="Resume processor not initialized")
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/resume/search")
async def search_resume_content(query: str, user_id: Optional[str] = None, limit: int = 5):
    """Search resume content using vector search"""
    try:
        if resume_processor:
            results = await resume_processor.search_resume_content(query, user_id, limit)
            return {"results": results}
        else:
            raise HTTPException(status_code=500, detail="Resume processor not initialized")
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001) 